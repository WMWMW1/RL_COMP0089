{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_world.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, grid_matrix, default_reward=-1, terminal_rewards=None, gamma=0.9):\n",
    "        \"\"\"\n",
    "        初始化网格世界环境。\n",
    "\n",
    "        :param grid_matrix: n x m 的矩阵，定义网格的结构和奖励。\n",
    "                            使用以下规则定义矩阵元素：\n",
    "                            - 'S' : 起始位置\n",
    "                            - 'G' : 目标位置\n",
    "                            - 'X' : 障碍物\n",
    "                            - 数值 : 特定位置的奖励\n",
    "                            - 'O' 或其他字符 : 默认奖励\n",
    "        :param default_reward: 默认步进奖励，默认为-1。\n",
    "        :param terminal_rewards: 字典，指定特定位置的终止奖励。\n",
    "        :param gamma: 折扣因子\n",
    "        \"\"\"\n",
    "        self.grid_matrix = grid_matrix\n",
    "        self.grid_size = (len(grid_matrix), len(grid_matrix[0]))\n",
    "        self.default_reward = default_reward\n",
    "        self.gamma = gamma\n",
    "        self.terminal_rewards = terminal_rewards if terminal_rewards else {}\n",
    "        \n",
    "        # 动作空间：0: 上, 1: 右, 2: 下, 3: 左\n",
    "        self.action_space = 4\n",
    "        self.state_space = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        # 解析网格矩阵，找到起始位置和目标位置\n",
    "        self.start = None\n",
    "        self.goal = None\n",
    "        self.obstacles = []\n",
    "        self.reward_map = {}  # {state: reward}\n",
    "\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                cell = grid_matrix[i][j]\n",
    "                state = self._pos_to_state((i, j))\n",
    "                if cell == 'S':\n",
    "                    self.start = (i, j)\n",
    "                elif cell == 'G':\n",
    "                    self.goal = (i, j)\n",
    "                    self.reward_map[state] = self.terminal_rewards.get((i, j), 10)\n",
    "                elif cell == 'X':\n",
    "                    self.obstacles.append((i, j))\n",
    "                elif isinstance(cell, (int, float)):\n",
    "                    self.reward_map[state] = cell\n",
    "\n",
    "        if self.start is None:\n",
    "            raise ValueError(\"Grid must have a start position marked with 'S'.\")\n",
    "        if self.goal is None:\n",
    "            raise ValueError(\"Grid must have a goal position marked with 'G'.\")\n",
    "\n",
    "        # 初始化 agent_pos 和 done 状态\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境到起始状态。\n",
    "\n",
    "        :return: 初始状态的整数表示。\n",
    "        \"\"\"\n",
    "        self.agent_pos = self.start\n",
    "        self.done = False\n",
    "        return self._pos_to_state(self.agent_pos)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作，返回下一个状态、奖励、是否结束以及额外信息。\n",
    "\n",
    "        :param action: 动作，整数0-3。\n",
    "        :return: next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            raise Exception(\"Episode has finished. Please reset the environment.\")\n",
    "\n",
    "        # 计算新位置\n",
    "        new_pos = self._move(self.agent_pos, action)\n",
    "\n",
    "        # 检查是否撞墙或进入障碍物\n",
    "        if self._is_valid(new_pos):\n",
    "            self.agent_pos = new_pos\n",
    "        else:\n",
    "            # 撞墙或障碍物，位置不变\n",
    "            pass\n",
    "\n",
    "        # 检查是否达到终止状态\n",
    "        state = self._pos_to_state(self.agent_pos)\n",
    "        if self.agent_pos == self.goal or self.agent_pos in self.terminal_rewards:\n",
    "            reward = self.reward_map.get(state, self.default_reward)\n",
    "            self.done = True\n",
    "        else:\n",
    "            reward = self.reward_map.get(state, self.default_reward)\n",
    "            self.done = False\n",
    "\n",
    "        next_state = state\n",
    "        info = {}\n",
    "\n",
    "        return next_state, reward, self.done, info\n",
    "\n",
    "    def _move(self, position, action):\n",
    "        \"\"\"\n",
    "        根据动作计算新位置。\n",
    "\n",
    "        :param position: 当前的位置，元组(x, y)。\n",
    "        :param action: 动作，整数0-3。\n",
    "        :return: 新的位置，元组(x, y)。\n",
    "        \"\"\"\n",
    "        x, y = position\n",
    "        if action == 0:  # 上\n",
    "            x -= 1\n",
    "        elif action == 1:  # 右\n",
    "            y += 1\n",
    "        elif action == 2:  # 下\n",
    "            x += 1\n",
    "        elif action == 3:  # 左\n",
    "            y -= 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "\n",
    "        return (x, y)\n",
    "\n",
    "    def _is_valid(self, position):\n",
    "        \"\"\"\n",
    "        检查位置是否有效（在网格内且不是障碍物）。\n",
    "\n",
    "        :param position: 位置，元组(x, y)。\n",
    "        :return: 如果有效返回True，否则返回False。\n",
    "        \"\"\"\n",
    "        x, y = position\n",
    "        # 检查边界\n",
    "        if x < 0 or x >= self.grid_size[0] or y < 0 or y >= self.grid_size[1]:\n",
    "            return False\n",
    "        # 检查障碍物\n",
    "        if position in self.obstacles:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _pos_to_state(self, position):\n",
    "        \"\"\"\n",
    "        将位置转换为状态编号。\n",
    "\n",
    "        :param position: 位置，元组(x, y)。\n",
    "        :return: 状态编号，整数。\n",
    "        \"\"\"\n",
    "        x, y = position\n",
    "        return x * self.grid_size[1] + y\n",
    "\n",
    "    def _state_to_pos(self, state):\n",
    "        \"\"\"\n",
    "        将状态编号转换为位置。\n",
    "\n",
    "        :param state: 状态编号，整数。\n",
    "        :return: 位置，元组(x, y)。\n",
    "        \"\"\"\n",
    "        x = state // self.grid_size[1]\n",
    "        y = state % self.grid_size[1]\n",
    "        return (x, y)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        打印当前网格的状态。\n",
    "        \"\"\"\n",
    "        grid = [['O' for _ in range(self.grid_size[1])] for _ in range(self.grid_size[0])]\n",
    "\n",
    "        # 设置障碍物\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0]][obs[1]] = 'X'\n",
    "\n",
    "        # 设置奖励位置\n",
    "        for state, reward in self.reward_map.items():\n",
    "            pos = self._state_to_pos(state)\n",
    "            if grid[pos[0]][pos[1]] == 'O':\n",
    "                grid[pos[0]][pos[1]] = f\"{reward}\"\n",
    "\n",
    "        # 设置目标\n",
    "        grid[self.goal[0]][self.goal[1]] = 'G'\n",
    "\n",
    "        # 设置智能体\n",
    "        grid[self.agent_pos[0]][self.agent_pos[1]] = 'A'\n",
    "\n",
    "        # 打印网格\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_step(env, state, action):\n",
    "    \"\"\"\n",
    "    模拟在特定状态下执行动作的结果。\n",
    "\n",
    "    :param env: 环境对象。\n",
    "    :param state: 当前状态。\n",
    "    :param action: 动作。\n",
    "    :return: next_state, reward, done, info\n",
    "    \"\"\"\n",
    "    # 保存当前状态\n",
    "    original_pos = env.agent_pos\n",
    "    env.agent_pos = env._state_to_pos(state)\n",
    "\n",
    "    # 执行动作\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    # 恢复原始状态\n",
    "    env.agent_pos = original_pos\n",
    "    env.done = False\n",
    "\n",
    "    return next_state, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "价值函数 V(s) 在第 1 次迭代前的值:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "开始第 1 次迭代\n",
      "评估状态 0 位置 (0, 0)\n",
      "  动作 0: 下一状态 0, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 1: 下一状态 1, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 5, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 0, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  状态 0 的最大动作价值: -1.00\n",
      "  更新前 V[0] = 0.00, 更新后 V[0] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "评估状态 1 位置 (0, 1)\n",
      "  动作 0: 下一状态 1, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 1: 下一状态 2, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 1, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 0, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 1 的最大动作价值: -1.00\n",
      "  更新前 V[1] = 0.00, 更新后 V[1] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "评估状态 2 位置 (0, 2)\n",
      "  动作 0: 下一状态 2, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 1: 下一状态 3, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 7, 奖励 = -5, 价值 = 0.00, Q值 = -5.00\n",
      "  动作 3: 下一状态 1, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 2 的最大动作价值: -1.00\n",
      "  更新前 V[2] = 0.00, 更新后 V[2] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1. -1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "评估状态 3 位置 (0, 3)\n",
      "  动作 0: 下一状态 3, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 1: 下一状态 4, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 3, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 2, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 3 的最大动作价值: -1.00\n",
      "  更新前 V[3] = 0.00, 更新后 V[3] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1. -1. -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "评估状态 4 位置 (0, 4)\n",
      "  动作 0: 下一状态 4, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 1: 下一状态 4, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 9, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 3, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 4 的最大动作价值: -1.00\n",
      "  更新前 V[4] = 0.00, 更新后 V[4] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1. -1. -1. -1. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "评估状态 5 位置 (1, 0)\n",
      "  动作 0: 下一状态 0, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 5, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 10, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 5, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  状态 5 的最大动作价值: -1.00\n",
      "  更新前 V[5] = 0.00, 更新后 V[5] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1. -1. -1. -1. -1.]\n",
      " [-1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "状态 6 位置 (1, 1) 是障碍物，跳过\n",
      "评估状态 7 位置 (1, 2)\n",
      "  动作 0: 下一状态 2, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 7, 奖励 = -5, 价值 = 0.00, Q值 = -5.00\n",
      "  动作 2: 下一状态 7, 奖励 = -5, 价值 = 0.00, Q值 = -5.00\n",
      "  动作 3: 下一状态 7, 奖励 = -5, 价值 = 0.00, Q值 = -5.00\n",
      "  状态 7 的最大动作价值: -1.90\n",
      "  更新前 V[7] = 0.00, 更新后 V[7] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]]\n",
      "状态 8 位置 (1, 3) 是障碍物，跳过\n",
      "评估状态 9 位置 (1, 4)\n",
      "  动作 0: 下一状态 4, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 9, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 14, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 9, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  状态 9 的最大动作价值: -1.00\n",
      "  更新前 V[9] = 0.00, 更新后 V[9] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]]\n",
      "评估状态 10 位置 (2, 0)\n",
      "  动作 0: 下一状态 5, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 11, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 15, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 10, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  状态 10 的最大动作价值: -1.00\n",
      "  更新前 V[10] = 0.00, 更新后 V[10] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]]\n",
      "评估状态 11 位置 (2, 1)\n",
      "  动作 0: 下一状态 11, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 1: 下一状态 11, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 16, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 10, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 11 的最大动作价值: -1.00\n",
      "  更新前 V[11] = 0.00, 更新后 V[11] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]]\n",
      "状态 12 位置 (2, 2) 是障碍物，跳过\n",
      "评估状态 13 位置 (2, 3)\n",
      "  动作 0: 下一状态 13, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 1: 下一状态 14, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 13, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 13, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  状态 13 的最大动作价值: -1.00\n",
      "  更新前 V[13] = 0.00, 更新后 V[13] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]]\n",
      "评估状态 14 位置 (2, 4)\n",
      "  动作 0: 下一状态 9, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 14, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 19, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 13, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 14 的最大动作价值: -1.00\n",
      "  更新前 V[14] = 0.00, 更新后 V[14] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]]\n",
      "评估状态 15 位置 (3, 0)\n",
      "  动作 0: 下一状态 10, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 16, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 20, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 15, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  状态 15 的最大动作价值: -1.00\n",
      "  更新前 V[15] = 0.00, 更新后 V[15] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]]\n",
      "评估状态 16 位置 (3, 1)\n",
      "  动作 0: 下一状态 11, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 17, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 21, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 15, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 16 的最大动作价值: -1.00\n",
      "  更新前 V[16] = 0.00, 更新后 V[16] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]]\n",
      "评估状态 17 位置 (3, 2)\n",
      "  动作 0: 下一状态 17, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 1: 下一状态 17, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 22, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 16, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 17 的最大动作价值: -1.00\n",
      "  更新前 V[17] = 0.00, 更新后 V[17] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]]\n",
      "状态 18 位置 (3, 3) 是障碍物，跳过\n",
      "评估状态 19 位置 (3, 4)\n",
      "  动作 0: 下一状态 14, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 19, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2 导致终止状态: 奖励 = 10\n",
      "  动作 3: 下一状态 19, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  状态 19 的最大动作价值: 10.00\n",
      "  更新前 V[19] = 0.00, 更新后 V[19] = 10.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [ 0.   0.   0.   0.   0. ]]\n",
      "评估状态 20 位置 (4, 0)\n",
      "  动作 0: 下一状态 15, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 21, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 20, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 20, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  状态 20 的最大动作价值: -1.00\n",
      "  更新前 V[20] = 0.00, 更新后 V[20] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.   0.   0.   0.   0. ]]\n",
      "评估状态 21 位置 (4, 1)\n",
      "  动作 0: 下一状态 16, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 22, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 21, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 20, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 21 的最大动作价值: -1.00\n",
      "  更新前 V[21] = 0.00, 更新后 V[21] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.   0.   0.   0. ]]\n",
      "评估状态 22 位置 (4, 2)\n",
      "  动作 0: 下一状态 17, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 23, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 2: 下一状态 22, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 21, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 22 的最大动作价值: -1.00\n",
      "  更新前 V[22] = 0.00, 更新后 V[22] = -1.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.   0.   0. ]]\n",
      "评估状态 23 位置 (4, 3)\n",
      "  动作 0: 下一状态 23, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 1 导致终止状态: 奖励 = 10\n",
      "  动作 2: 下一状态 23, 奖励 = -1, 价值 = 0.00, Q值 = -1.00\n",
      "  动作 3: 下一状态 22, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 23 的最大动作价值: 10.00\n",
      "  更新前 V[23] = 0.00, 更新后 V[23] = 10.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.  10.   0. ]]\n",
      "评估状态 24 位置 (4, 4)\n",
      "  动作 0: 下一状态 19, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
      "  动作 1 导致终止状态: 奖励 = 10\n",
      "  动作 2 导致终止状态: 奖励 = 10\n",
      "  动作 3: 下一状态 23, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
      "  状态 24 的最大动作价值: 10.00\n",
      "  更新前 V[24] = 0.00, 更新后 V[24] = 10.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.  10.  10. ]]\n",
      "第 1 次迭代完成，delta=10.000000\n",
      "价值函数 V(s) 在第 1 次迭代后的值:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.  10.  10. ]]\n"
     ]
    }
   ],
   "source": [
    "grid_matrix = [\n",
    "    ['S', 'O', 'O', 'O', 'O'],\n",
    "    ['O', 'X',  -5, 'X', 'O'],\n",
    "    ['O', 'O', 'X', 'O', 'O'],\n",
    "    ['O', 'O', 'O', 'X', 'O'],\n",
    "    ['O', 'O', 'O', 'O', 'G']\n",
    "]\n",
    "\n",
    "# 定义终止奖励（例如目标位置）\n",
    "terminal_rewards = {\n",
    "    (4, 4): 10\n",
    "}\n",
    "\n",
    "# 创建 GridWorld 环境\n",
    "env = GridWorld(\n",
    "    grid_matrix=grid_matrix,\n",
    "    default_reward=-1,\n",
    "    terminal_rewards=terminal_rewards,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "# 初始化价值函数 V(s) = 0\n",
    "V = np.zeros(env.state_space)\n",
    "gamma = env.gamma\n",
    "\n",
    "# 设置收敛阈值和最大迭代次数\n",
    "theta = 1e-4\n",
    "max_iterations = 1000\n",
    "\n",
    "# 进行一次迭代\n",
    "i = 0\n",
    "delta = 0\n",
    "state = 0  # 示例状态\n",
    "print(\"价值函数 V(s) 在第 1 次迭代前的值:\")\n",
    "print(V.reshape(env.grid_size))\n",
    "print(f\"开始第 {i + 1} 次迭代\")\n",
    "for state in range(env.state_space):\n",
    "    pos = env._state_to_pos(state)\n",
    "\n",
    "    # 跳过障碍物\n",
    "    if pos in env.obstacles:\n",
    "        print(f\"状态 {state} 位置 {pos} 是障碍物，跳过\")\n",
    "        continue\n",
    "\n",
    "    # 计算所有动作的价值\n",
    "    action_values = []\n",
    "    print(f\"评估状态 {state} 位置 {pos}\")\n",
    "    for action in range(env.action_space):\n",
    "        next_state, reward, done, _ = simulate_step(env, state, action)\n",
    "        if done:\n",
    "            action_value = reward\n",
    "            print(f\"  动作 {action} 导致终止状态: 奖励 = {reward}\")\n",
    "        else:\n",
    "            action_value = reward + gamma * V[next_state]\n",
    "            print(f\"  动作 {action}: 下一状态 {next_state}, 奖励 = {reward}, 价值 = {V[next_state]:.2f}, Q值 = {action_value:.2f}\")\n",
    "        action_values.append(action_value)\n",
    "\n",
    "    # 更新价值函数\n",
    "    max_action_value = max(action_values)\n",
    "    print(f\"  状态 {state} 的最大动作价值: {max_action_value:.2f}\")\n",
    "    delta = max(delta, abs(max_action_value - V[state]))\n",
    "    print(f\"  更新前 V[{state}] = {V[state]:.2f}, 更新后 V[{state}] = {max_action_value:.2f}\\n\")\n",
    "    V[state] = max_action_value\n",
    "    print(\"grid_matrix:\")\n",
    "    print(V.reshape(env.grid_size))\n",
    "\n",
    "print(f\"第 {i + 1} 次迭代完成，delta={delta:.6f}\")\n",
    "\n",
    "# 显示价值函数\n",
    "print(\"价值函数 V(s) 在第 1 次迭代后的值:\")\n",
    "print(V.reshape(env.grid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设只存在value 函数，并且policy就只是选择value最大的那个选项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "价值函数 V(s) 在第 2 次迭代前的值:\n",
      "[[-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.  10.  10. ]]\n",
      "开始第 2 次迭代\n",
      "评估状态 0 位置 (0, 0)\n",
      "  动作 0: 下一状态 0, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 1, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 5, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 0, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 0 的最大动作价值: -1.90\n",
      "  更新前 V[0] = -1.00, 更新后 V[0] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9 -1.  -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.  10.  10. ]]\n",
      "评估状态 1 位置 (0, 1)\n",
      "  动作 0: 下一状态 1, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 2, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 1, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 0, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  状态 1 的最大动作价值: -1.90\n",
      "  更新前 V[1] = -1.00, 更新后 V[1] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9 -1.9 -1.  -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.  10.  10. ]]\n",
      "评估状态 2 位置 (0, 2)\n",
      "  动作 0: 下一状态 2, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 3, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 7, 奖励 = -5, 价值 = -1.90, Q值 = -6.71\n",
      "  动作 3: 下一状态 1, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  状态 2 的最大动作价值: -1.90\n",
      "  更新前 V[2] = -1.00, 更新后 V[2] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9 -1.9 -1.9 -1.  -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.  10.  10. ]]\n",
      "评估状态 3 位置 (0, 3)\n",
      "  动作 0: 下一状态 3, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 4, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 3, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 2, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  状态 3 的最大动作价值: -1.90\n",
      "  更新前 V[3] = -1.00, 更新后 V[3] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9 -1.9 -1.9 -1.9 -1. ]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.  10.  10. ]]\n",
      "评估状态 4 位置 (0, 4)\n",
      "  动作 0: 下一状态 4, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 4, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 9, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 3, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  状态 4 的最大动作价值: -1.90\n",
      "  更新前 V[4] = -1.00, 更新后 V[4] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9 -1.9 -1.9 -1.9 -1.9]\n",
      " [-1.   0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.  10.  10. ]]\n",
      "评估状态 5 位置 (1, 0)\n",
      "  动作 0: 下一状态 0, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  动作 1: 下一状态 5, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 10, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 5, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 5 的最大动作价值: -1.90\n",
      "  更新前 V[5] = -1.00, 更新后 V[5] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9 -1.9 -1.9 -1.9 -1.9]\n",
      " [-1.9  0.  -1.9  0.  -1. ]\n",
      " [-1.  -1.   0.  -1.  -1. ]\n",
      " [-1.  -1.  -1.   0.  10. ]\n",
      " [-1.  -1.  -1.  10.  10. ]]\n",
      "状态 6 位置 (1, 1) 是障碍物，跳过\n",
      "评估状态 7 位置 (1, 2)\n",
      "  动作 0: 下一状态 2, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  动作 1: 下一状态 7, 奖励 = -5, 价值 = -1.90, Q值 = -6.71\n",
      "  动作 2: 下一状态 7, 奖励 = -5, 价值 = -1.90, Q值 = -6.71\n",
      "  动作 3: 下一状态 7, 奖励 = -5, 价值 = -1.90, Q值 = -6.71\n",
      "  状态 7 的最大动作价值: -2.71\n",
      "  更新前 V[7] = -1.90, 更新后 V[7] = -2.71\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.  ]\n",
      " [-1.   -1.    0.   -1.   -1.  ]\n",
      " [-1.   -1.   -1.    0.   10.  ]\n",
      " [-1.   -1.   -1.   10.   10.  ]]\n",
      "状态 8 位置 (1, 3) 是障碍物，跳过\n",
      "评估状态 9 位置 (1, 4)\n",
      "  动作 0: 下一状态 4, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  动作 1: 下一状态 9, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 14, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 9, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 9 的最大动作价值: -1.90\n",
      "  更新前 V[9] = -1.00, 更新后 V[9] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.   -1.    0.   -1.   -1.  ]\n",
      " [-1.   -1.   -1.    0.   10.  ]\n",
      " [-1.   -1.   -1.   10.   10.  ]]\n",
      "评估状态 10 位置 (2, 0)\n",
      "  动作 0: 下一状态 5, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  动作 1: 下一状态 11, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 15, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 10, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 10 的最大动作价值: -1.90\n",
      "  更新前 V[10] = -1.00, 更新后 V[10] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.    0.   -1.   -1.  ]\n",
      " [-1.   -1.   -1.    0.   10.  ]\n",
      " [-1.   -1.   -1.   10.   10.  ]]\n",
      "评估状态 11 位置 (2, 1)\n",
      "  动作 0: 下一状态 11, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 11, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 16, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 10, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  状态 11 的最大动作价值: -1.90\n",
      "  更新前 V[11] = -1.00, 更新后 V[11] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.   -1.  ]\n",
      " [-1.   -1.   -1.    0.   10.  ]\n",
      " [-1.   -1.   -1.   10.   10.  ]]\n",
      "状态 12 位置 (2, 2) 是障碍物，跳过\n",
      "评估状态 13 位置 (2, 3)\n",
      "  动作 0: 下一状态 13, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 14, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 13, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 13, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 13 的最大动作价值: -1.90\n",
      "  更新前 V[13] = -1.00, 更新后 V[13] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9  -1.  ]\n",
      " [-1.   -1.   -1.    0.   10.  ]\n",
      " [-1.   -1.   -1.   10.   10.  ]]\n",
      "评估状态 14 位置 (2, 4)\n",
      "  动作 0: 下一状态 9, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  动作 1: 下一状态 14, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 19, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
      "  动作 3: 下一状态 13, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  状态 14 的最大动作价值: 8.00\n",
      "  更新前 V[14] = -1.00, 更新后 V[14] = 8.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.   -1.   -1.    0.   10.  ]\n",
      " [-1.   -1.   -1.   10.   10.  ]]\n",
      "评估状态 15 位置 (3, 0)\n",
      "  动作 0: 下一状态 10, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  动作 1: 下一状态 16, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 20, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 15, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 15 的最大动作价值: -1.90\n",
      "  更新前 V[15] = -1.00, 更新后 V[15] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.9  -1.   -1.    0.   10.  ]\n",
      " [-1.   -1.   -1.   10.   10.  ]]\n",
      "评估状态 16 位置 (3, 1)\n",
      "  动作 0: 下一状态 11, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  动作 1: 下一状态 17, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 21, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 15, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  状态 16 的最大动作价值: -1.90\n",
      "  更新前 V[16] = -1.00, 更新后 V[16] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.9  -1.9  -1.    0.   10.  ]\n",
      " [-1.   -1.   -1.   10.   10.  ]]\n",
      "评估状态 17 位置 (3, 2)\n",
      "  动作 0: 下一状态 17, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 1: 下一状态 17, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 22, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 16, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  状态 17 的最大动作价值: -1.90\n",
      "  更新前 V[17] = -1.00, 更新后 V[17] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.9  -1.9  -1.9   0.   10.  ]\n",
      " [-1.   -1.   -1.   10.   10.  ]]\n",
      "状态 18 位置 (3, 3) 是障碍物，跳过\n",
      "评估状态 19 位置 (3, 4)\n",
      "  动作 0: 下一状态 14, 奖励 = -1, 价值 = 8.00, Q值 = 6.20\n",
      "  动作 1: 下一状态 19, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
      "  动作 2 导致终止状态: 奖励 = 10\n",
      "  动作 3: 下一状态 19, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
      "  状态 19 的最大动作价值: 10.00\n",
      "  更新前 V[19] = 10.00, 更新后 V[19] = 10.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.9  -1.9  -1.9   0.   10.  ]\n",
      " [-1.   -1.   -1.   10.   10.  ]]\n",
      "评估状态 20 位置 (4, 0)\n",
      "  动作 0: 下一状态 15, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  动作 1: 下一状态 21, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 20, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 20, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  状态 20 的最大动作价值: -1.90\n",
      "  更新前 V[20] = -1.00, 更新后 V[20] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.9  -1.9  -1.9   0.   10.  ]\n",
      " [-1.9  -1.   -1.   10.   10.  ]]\n",
      "评估状态 21 位置 (4, 1)\n",
      "  动作 0: 下一状态 16, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  动作 1: 下一状态 22, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 2: 下一状态 21, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 20, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  状态 21 的最大动作价值: -1.90\n",
      "  更新前 V[21] = -1.00, 更新后 V[21] = -1.90\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.9  -1.9  -1.9   0.   10.  ]\n",
      " [-1.9  -1.9  -1.   10.   10.  ]]\n",
      "评估状态 22 位置 (4, 2)\n",
      "  动作 0: 下一状态 17, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  动作 1: 下一状态 23, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
      "  动作 2: 下一状态 22, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
      "  动作 3: 下一状态 21, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
      "  状态 22 的最大动作价值: 8.00\n",
      "  更新前 V[22] = -1.00, 更新后 V[22] = 8.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.9  -1.9  -1.9   0.   10.  ]\n",
      " [-1.9  -1.9   8.   10.   10.  ]]\n",
      "评估状态 23 位置 (4, 3)\n",
      "  动作 0: 下一状态 23, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
      "  动作 1 导致终止状态: 奖励 = 10\n",
      "  动作 2: 下一状态 23, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
      "  动作 3: 下一状态 22, 奖励 = -1, 价值 = 8.00, Q值 = 6.20\n",
      "  状态 23 的最大动作价值: 10.00\n",
      "  更新前 V[23] = 10.00, 更新后 V[23] = 10.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.9  -1.9  -1.9   0.   10.  ]\n",
      " [-1.9  -1.9   8.   10.   10.  ]]\n",
      "评估状态 24 位置 (4, 4)\n",
      "  动作 0: 下一状态 19, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
      "  动作 1 导致终止状态: 奖励 = 10\n",
      "  动作 2 导致终止状态: 奖励 = 10\n",
      "  动作 3: 下一状态 23, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
      "  状态 24 的最大动作价值: 10.00\n",
      "  更新前 V[24] = 10.00, 更新后 V[24] = 10.00\n",
      "\n",
      "grid_matrix:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.9  -1.9  -1.9   0.   10.  ]\n",
      " [-1.9  -1.9   8.   10.   10.  ]]\n",
      "第 2 次迭代完成，delta=9.000000\n",
      "价值函数 V(s) 在第 2 次迭代后的值:\n",
      "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
      " [-1.9   0.   -2.71  0.   -1.9 ]\n",
      " [-1.9  -1.9   0.   -1.9   8.  ]\n",
      " [-1.9  -1.9  -1.9   0.   10.  ]\n",
      " [-1.9  -1.9   8.   10.   10.  ]]\n"
     ]
    }
   ],
   "source": [
    "# secont iteration\n",
    "\n",
    "# 进行第二次迭代\n",
    "i += 1\n",
    "delta = 0\n",
    "print(\"价值函数 V(s) 在第 2 次迭代前的值:\")\n",
    "print(V.reshape(env.grid_size))\n",
    "print(f\"开始第 {i + 1} 次迭代\")\n",
    "for state in range(env.state_space):\n",
    "    pos = env._state_to_pos(state)\n",
    "\n",
    "    # 跳过障碍物\n",
    "    if pos in env.obstacles:\n",
    "        print(f\"状态 {state} 位置 {pos} 是障碍物，跳过\")\n",
    "        continue\n",
    "\n",
    "    # 计算所有动作的价值\n",
    "    action_values = []\n",
    "    print(f\"评估状态 {state} 位置 {pos}\")\n",
    "    for action in range(env.action_space):\n",
    "        next_state, reward, done, _ = simulate_step(env, state, action)\n",
    "        if done:\n",
    "            action_value = reward\n",
    "            print(f\"  动作 {action} 导致终止状态: 奖励 = {reward}\")\n",
    "        else:\n",
    "            action_value = reward + gamma * V[next_state]\n",
    "            print(f\"  动作 {action}: 下一状态 {next_state}, 奖励 = {reward}, 价值 = {V[next_state]:.2f}, Q值 = {action_value:.2f}\")\n",
    "        action_values.append(action_value)\n",
    "\n",
    "    # 更新价值函数\n",
    "    max_action_value = max(action_values)\n",
    "    print(f\"  状态 {state} 的最大动作价值: {max_action_value:.2f}\")\n",
    "    delta = max(delta, abs(max_action_value - V[state]))\n",
    "    print(f\"  更新前 V[{state}] = {V[state]:.2f}, 更新后 V[{state}] = {max_action_value:.2f}\\n\")\n",
    "    V[state] = max_action_value\n",
    "    print(\"grid_matrix:\")\n",
    "    print(V.reshape(env.grid_size))\n",
    "\n",
    "print(f\"第 {i + 1} 次迭代完成，delta={delta:.6f}\")\n",
    "\n",
    "# 显示价值函数\n",
    "print(\"价值函数 V(s) 在第 2 次迭代后的值:\")\n",
    "print(V.reshape(env.grid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以把代码对应到公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual value function is the expected return\n",
    "\n",
    "$$\n",
    "v^\\pi(s) = \\mathbb{E}[G_t \\mid S_t = s, \\pi] = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s, \\pi]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman Optimality Equation："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V^*(s) = \\max_a \\mathbb{E} \\left[ R_{t+1} + \\gamma V^*(S_{t+1}) \\mid S_t = s, A_t = a \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Expanding Expectation\n",
    "\n",
    "By definition of expectation:\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_a \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma V^*(s') \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( P(s' \\mid s, a) \\) is the transition probability from \\( s \\) to \\( s' \\) given action \\( a \\).\n",
    "- \\( R(s, a, s') \\) is the reward received when transitioning to \\( s' \\).\n",
    "- \\( V^*(s') \\) is the estimated value of the next state.\n",
    "\n",
    "### Deterministic Case\n",
    "\n",
    "If the environment is deterministic, then for a given \\( s, a \\), there is only one possible next state \\( s' \\). This simplifies the equation to:\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_a \\left[ R(s, a) + \\gamma V^*(s') \\right]\n",
    "$$\n",
    "\n",
    "### Iterative Approximation\n",
    "\n",
    "To approximate \\( V^*(s) \\), we define an iterative update:\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) = \\max_a \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma V_k(s') \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ V_k(s) $ is the value function at iteration k\n",
    "- We iterate until convergence, where $V_{k+1}(s) \\approx V_k(s) $\n",
    "\n",
    "This forms the basis of the **Value Iteration Algorithm**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "grid_matrix:\n",
    "[[-1.9  -1.9  -1.9  -1.9  -1.9 ]\n",
    " [-1.9   0.   -2.71  0.   -1.9 ]\n",
    " [-1.9  -1.9   0.   -1.9   8.  ]\n",
    " [-1.9  -1.9  -1.9   0.   10.  ]\n",
    " [-1.9  -1.9  -1.   10.   10.  ]]\n",
    "评估状态 22 位置 (4, 2)\n",
    "  动作 0: 下一状态 17, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
    "  动作 1: 下一状态 23, 奖励 = -1, 价值 = 10.00, Q值 = 8.00\n",
    "  动作 2: 下一状态 22, 奖励 = -1, 价值 = -1.00, Q值 = -1.90\n",
    "  动作 3: 下一状态 21, 奖励 = -1, 价值 = -1.90, Q值 = -2.71\n",
    "  状态 22 的最大动作价值: 8.00\n",
    "  更新前 V[22] = -1.00, 更新后 V[22] = 8.00\n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中一个print\n",
    "更新了-1到 8的迭代\n",
    "\n",
    "对于动作0 $R_{t+1}=-1 V(S_{t+1})=1.90$\n",
    "\n",
    "对于动作1 $R_{t+1}=-1 V(S_{t+1})=10$ \n",
    "\n",
    "对于动作2 $R_{t+1}=-1 V(S_{t+1})=-1$\n",
    "\n",
    "对于动作3 $R_{t+1}=-1 V(S_{t+1})=-1.90$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward + gamma * V[next_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中最大的是动作1，所以把V(s)更新为-1+10*0.9(gamma)=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Value Iteration Convergence\n",
    "\n",
    "### **1. Bellman Optimality Operator**\n",
    "Define the Bellman optimality operator $\\mathcal{T}^*$:\n",
    "\n",
    "$$\n",
    "(\\mathcal{T}^*[V])(s) = \\max_a \\sum_{s'} P(s' \\mid s,a) \n",
    "\\Big( R(s,a,s') + \\gamma V(s') \\Big).\n",
    "$$\n",
    "\n",
    "### **2. Contraction Property**\n",
    "For any two value functions $V_1$ and $V_2$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bigl\\lVert \\mathcal{T}^*[V_1] - \\mathcal{T}^*[V_2] \\bigr\\rVert_\\infty\n",
    "&= \\max_{s} \\Bigl\\lvert \\max_a \\sum_{s'} P(s' \\mid s,a)\\bigl(R + \\gamma V_1(s')\\bigr) \\\\\n",
    "&\\quad - \\max_a \\sum_{s'} P(s' \\mid s,a)\\bigl(R + \\gamma V_2(s')\\bigr) \\Bigr\\rvert.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using the **triangle inequality** and properties of the max operator, we get:\n",
    "\n",
    "$$\n",
    "\\bigl\\lVert \\mathcal{T}^*[V_1] - \\mathcal{T}^*[V_2] \\bigr\\rVert_\\infty\n",
    "\\leq \\gamma\\, \\bigl\\lVert V_1 - V_2 \\bigr\\rVert_\\infty.\n",
    "$$\n",
    "\n",
    "Since $0 \\leq \\gamma < 1$, $\\mathcal{T}^*$ is a **contraction mapping**.\n",
    "\n",
    "### **3. Existence of Unique Fixed Point $V^*$**\n",
    "Define $V^*$ as the unique fixed point of $\\mathcal{T}^*$:\n",
    "\n",
    "$$\n",
    "\\mathcal{T}^*[V^*] = V^*.\n",
    "$$\n",
    "\n",
    "The existence and uniqueness of $V^*$ follows from the **Banach Fixed-Point Theorem**.\n",
    "\n",
    "### **4. Convergence of Value Iteration**\n",
    "Starting from any initial $V_0$, the iteration:\n",
    "\n",
    "$$\n",
    "V_{k+1} = \\mathcal{T}^*[V_k]\n",
    "$$\n",
    "\n",
    "produces a sequence $V_0, V_1, V_2, \\dots$ that converges to $V^*$ as $k \\to \\infty$.\n",
    "\n",
    "Thus, **Value Iteration approximates $V^*$ as the number of iterations increases**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: delta=10.0\n",
      "Iteration 2: delta=9.0\n",
      "Iteration 3: delta=8.1\n",
      "Iteration 4: delta=7.29\n",
      "Iteration 5: delta=6.561\n",
      "Iteration 6: delta=5.9049000000000005\n",
      "Iteration 7: delta=5.3144100000000005\n",
      "Iteration 8: delta=4.7829690000000005\n",
      "Iteration 9: delta=0\n",
      "价值迭代收敛\n",
      "最优价值函数:\n",
      "[[-0.434062  0.62882   1.8098    3.122     4.58    ]\n",
      " [ 0.62882   0.        0.62882   0.        6.2     ]\n",
      " [ 1.8098    3.122     0.        6.2       8.      ]\n",
      " [ 3.122     4.58      6.2       0.       10.      ]\n",
      " [ 4.58      6.2       8.       10.       10.      ]]\n",
      "最优策略:\n",
      "[['→' '→' '→' '→' '↓']\n",
      " ['↓' 'X' '↑' 'X' '↓']\n",
      " ['→' '↓' 'X' '→' '↓']\n",
      " ['→' '→' '↓' 'X' '↓']\n",
      " ['→' '→' '→' '→' '→']]\n",
      "\n",
      "策略可视化:\n",
      " S  →  →  →  ↓ \n",
      " ↓  X  ↑  X  ↓ \n",
      " →  ↓  X  →  ↓ \n",
      " →  →  ↓  X  ↓ \n",
      " →  →  →  →  G \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def value_iteration(env, theta=1e-4, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    价值迭代算法。\n",
    "\n",
    "    :param env: 环境对象。\n",
    "    :param theta: 收敛阈值。\n",
    "    :param max_iterations: 最大迭代次数。\n",
    "    :return: 最优价值函数和最优策略。\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.state_space)\n",
    "    gamma = env.gamma\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0\n",
    "        for state in range(env.state_space):\n",
    "            pos = env._state_to_pos(state)\n",
    "\n",
    "            # 跳过障碍物\n",
    "            if pos in env.obstacles:\n",
    "                continue\n",
    "\n",
    "            # 计算所有动作的价值\n",
    "            action_values = []\n",
    "            for action in range(env.action_space):\n",
    "                next_state, reward, done, _ = simulate_step(env, state, action)\n",
    "                action_value = reward + gamma * V[next_state] if not done else reward\n",
    "                action_values.append(action_value)\n",
    "\n",
    "            # 更新价值函数\n",
    "            max_action_value = max(action_values)\n",
    "            delta = max(delta, abs(max_action_value - V[state]))\n",
    "            V[state] = max_action_value\n",
    "\n",
    "        print(f\"Iteration {i + 1}: delta={delta}\")\n",
    "        if delta < theta:\n",
    "            print(\"价值迭代收敛\")\n",
    "            break\n",
    "\n",
    "    # 导出最优策略\n",
    "    policy = np.zeros(env.state_space, dtype=int)\n",
    "    for state in range(env.state_space):\n",
    "        pos = env._state_to_pos(state)\n",
    "\n",
    "        # 跳过障碍物\n",
    "        if pos in env.obstacles:\n",
    "            policy[state] = -1  # 表示无效动作\n",
    "            continue\n",
    "\n",
    "        # 选择最佳动作\n",
    "        action_values = []\n",
    "        for action in range(env.action_space):\n",
    "            next_state, reward, done, _ = simulate_step(env, state, action)\n",
    "            action_value = reward + gamma * V[next_state] if not done else reward\n",
    "            action_values.append(action_value)\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[state] = best_action\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "\n",
    "\n",
    "grid_matrix = [\n",
    "    ['S', 'O', 'O', 'O', 'O'],\n",
    "    ['O', 'X',  -5, 'X', 'O'],\n",
    "    ['O', 'O', 'X', 'O', 'O'],\n",
    "    ['O', 'O', 'O', 'X', 'O'],\n",
    "    ['O', 'O', 'O', 'O', 'G']\n",
    "]\n",
    "\n",
    "# 定义终止奖励（例如目标位置）\n",
    "terminal_rewards = {\n",
    "    (4, 4): 10\n",
    "}\n",
    "\n",
    "env = GridWorld(\n",
    "    grid_matrix=grid_matrix,\n",
    "    default_reward=-1,\n",
    "    terminal_rewards=terminal_rewards,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "V, policy = value_iteration(env)\n",
    "\n",
    "print(\"最优价值函数:\")\n",
    "print(V.reshape(env.grid_size))\n",
    "\n",
    "print(\"最优策略:\")\n",
    "action_mapping = {0: '↑', 1: '→', 2: '↓', 3: '←', -1: 'X'}\n",
    "policy_grid = []\n",
    "for state in range(env.state_space):\n",
    "    action = policy[state]\n",
    "    policy_grid.append(action_mapping.get(action, '?'))\n",
    "policy_grid = np.array(policy_grid).reshape(env.grid_size)\n",
    "print(policy_grid)\n",
    "\n",
    "# 可视化策略\n",
    "print(\"\\n策略可视化:\")\n",
    "for i in range(env.grid_size[0]):\n",
    "    row = \"\"\n",
    "    for j in range(env.grid_size[1]):\n",
    "        state = env._pos_to_state((i, j))\n",
    "        if (i, j) in env.obstacles:\n",
    "            row += \" X \"\n",
    "        elif (i, j) == env.goal:\n",
    "            row += \" G \"\n",
    "        elif (i, j) == env.start:\n",
    "            row += \" S \"\n",
    "        else:\n",
    "            action = policy[state]\n",
    "            row += f\" {action_mapping.get(action, '?')} \"\n",
    "    print(row)\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_HF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
